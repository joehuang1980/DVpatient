{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hyperopt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8d2b5c83e051>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhyperopt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTATUS_OK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hyperopt'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from keras.datasets import mnist\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"tensorflow version:\", tf.VERSION)\n",
    "print(\"tensorflow keras version:\", tf.keras.__version__)\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "def auc_roc(y_true, y_pred):  \n",
    "    # any tensorflow metric\n",
    "    value, update_op = tf.metrics.auc(y_true, y_pred, num_thresholds=200, curve='ROC', summation_method='careful_interpolation' )\n",
    "    # find all variables created for this metric\n",
    "    metric_vars = [i for i in tf.local_variables() if 'auc_roc' in i.name.split('/')[1]]\n",
    "    # Add metric variables to GLOBAL_VARIABLES collection.\n",
    "    # They will be initialized for new session.\n",
    "    for v in metric_vars:\n",
    "        tf.add_to_collection(tf.GraphKeys.GLOBAL_VARIABLES, v)\n",
    "    # force to update metric values\n",
    "    with tf.control_dependencies([update_op]):\n",
    "        value = tf.identity(value)\n",
    "        return value\n",
    "\n",
    "\n",
    "def PreprocessData (raw_df):\n",
    "    ndarray=raw_df.values\n",
    "    Label=ndarray[:,0]\n",
    "    Features=raw_df.drop(['Severity'], axis=1)\n",
    "    \n",
    "    minmax_scale=preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "    scaleFeatures=minmax_scale.fit_transform(Features)\n",
    "    \n",
    "    return scaleFeatures, Label\n",
    "    \n",
    "\n",
    "def data():\n",
    "    df=pd.read_csv('dengue data-1070801codedSM.csv', encoding = 'utf8')\n",
    "    df=df.drop(['date', 'RNA'],  axis=1)\n",
    "    df=df.dropna(how='any')\n",
    "    train_df, test_df = train_test_split(df, test_size=.1, \n",
    "                                         stratify=df.Severity, random_state=42)\n",
    "    ndarray_train=train_df.values\n",
    "    Label_train=ndarray_train[:,0]\n",
    "    Features_train=train_df.drop(['Severity'], axis=1)\n",
    "    minmax_scale=preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "    scaleFeatures_train=minmax_scale.fit_transform(Features_train)\n",
    "    train_Features=scaleFeatures_train\n",
    "    train_Label=Label_train\n",
    "    \n",
    "    ndarray_test=test_df.values\n",
    "    Label_test=ndarray_test[:,0]\n",
    "    Features_test=test_df.drop(['Severity'], axis=1)\n",
    "    minmax_scale=preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "    scaleFeatures_test=minmax_scale.fit_transform(Features_test)\n",
    "    test_Features=scaleFeatures_test\n",
    "    test_Label=Label_test\n",
    "    \n",
    "    \n",
    "    \n",
    "    x_train = train_Features\n",
    "    x_test = test_Features\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    y_train = train_Label\n",
    "    y_test = test_Label\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "def create_model(x_train, y_train, x_test, y_test):\n",
    "    import keras\n",
    "    import keras_metrics as km\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense({{choice([100, 200, 300, 400])}}, input_shape=(5,)))\n",
    "    model.add(Activation({{choice(['relu', 'sigmoid'])}}))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    model.add(Dense({{choice([100, 200, 300, 400])}}))\n",
    "    model.add(Activation({{choice(['relu', 'sigmoid'])}}))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "\n",
    "    # If we choose 'four', add an additional fourth layer\n",
    "    if {{choice(['three', 'four'])}} == 'four':\n",
    "        model.add(Dense({{choice([100, 200, 400])}}))\n",
    "\n",
    "        # We can also choose between complete sets of layers\n",
    "        #model.add(space['add'])\n",
    "        model.add(Activation({{choice(['relu', 'sigmoid'])}}))\n",
    "        model.add(Dropout({{uniform(0, 1)}}))\n",
    "        \n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation({{choice(['relu', 'sigmoid'])}}))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', metrics=[auc_roc],\n",
    "                  optimizer={{choice(['rmsprop', 'adam', 'sgd'])}})\n",
    "\n",
    "    class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)\n",
    "    \n",
    "    \n",
    "  \n",
    "    result = model.fit(x_train, y_train,\n",
    "              batch_size={{choice([50,100,200,400])}},\n",
    "              epochs={{choice([400,500,600,700,800])}},\n",
    "              verbose=0,\n",
    "              validation_split={{choice([0.1,0.2,0.3,0.4,0.5])}},\n",
    "              class_weight=class_weights\n",
    "                       \n",
    "                      )\n",
    "    #get the highest validation accuracy of the training epochs\n",
    "    print(result.history.keys())\n",
    "    validation_recall = np.amax(result.history['val_auc_roc']) \n",
    "    print('Best validation recall of epoch:', validation_auc_roc)\n",
    "    return {'loss': -validation_auc_roc, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    best_run, best_model = optim.minimize(model=create_model,\n",
    "                                          data=data,\n",
    "                                          algo=tpe.suggest,\n",
    "                                          max_evals=20,\n",
    "                                          trials=Trials(),\n",
    "                                          notebook_name='Hyperparameter_ANNmodel20190416-OK-2-ROC_AUC',\n",
    "                                          eval_space=True \n",
    "                                         )\n",
    "    X_train, Y_train, X_test, Y_test = data()\n",
    "    print(\"Evalutation of best performing model:\")\n",
    "    print(best_model.evaluate(X_test, Y_test))\n",
    "    print(\"Best performing model chosen hyper-parameters:\")\n",
    "    print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
